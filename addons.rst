.. important::
    You are viewing an outdated version of the docs. Please find the latest version `here <http://doc.scrapinghub.com/>`_.

.. _addons:

======
Addons
======

Addons allow to extend bot capabilities and configure them easily from Scrapinghub. Each addon provides a particular feature. There are two flavours of addons: builtin addons and user addons. Builtin addons are always enabled and cannot be disabled by users.

Addons are very popular among Autoscraping users as a way of obtaining additional functionality without writing any code. Scrapy Cloud users often tend to write the extensions themselves. In fact, addons are no more than a fancy UI to configure `Scrapy`_ extensions.

To put an addon to use, you have to:

    #. Add it to your project, by going to **Settings -> Addons** in the Scrapinghub dashboard.
    #. Enable it. Some addons are enabled automatically when added, while others require configuring (e.g. editing ``ADDON_ENABLED`` setting). It should be clear from the addon page in the dashboard which case it is.

You can also enable addons per spider, instead of project-wide: select the spider in **Spiders** section, click **Settings** tab (next to **Details**), click ``+`` button in the corresponding addon field to add a new entry and choose ``ADDON_ENABLED`` from the list of options.

.. _autothrottle-addon:

Autothrottle
============

Allows the bot to crawl the target site with more caution, by dynamically adjusting request concurrency and delay according to the site lag and user control parameters. For details see `Scrapy Autothrottle`_ documentation.

This addon is loaded by default in every project. The basic settings controlling its behaviour are:

* ``CONCURRENT_REQUESTS_PER_DOMAIN`` - limits the maximum number of concurrent requests sent to the same host domain (default value is ``8``)
* ``DOWNLOAD_DELAY`` - limits the minimum download delay (in seconds) between each burst of requests (default value is ``0``)
* ``AUTOTHROTTLE_ENABLED`` - enables or disables *Autothrottle* addon (default value is ``True``, i.e. enabled)

**How to adjust these parameters?** The settings greatly depend on the user's needs, there are no values that will work for any target server. The default values are in general a good starting point and most servers tolerate them. Still there's a possibility of blocking and a need to slow down the crawling rate may emerge. Or quite the contrary, you may want the bot to crawl faster, in such instance you should fully realize that the risk of blocking increases. 

The crawling rate may be slowed down by adjusting the maximum concurrency ``CONCURRENT_REQUESTS_PER_DOMAIN`` to ``1``, and increasing the minimum download delay ``DOWNLOAD_DELAY`` at will. Regarding the maximum effective crawling rate, in practice it will be limited to the target server response rate, but may try to
speed it up by randomly increasing maximum concurrency (although in reality it produces no significant effect as concurrency will hardly exceed 2 for most sites).

As *Autothrottle* dynamically adjusts delay and concurrency depending on the site lag, the parameters only define limits while not forcing values. The minimum download delay value will not let the effective download delay take lower values during crawling, and the maximum concurrency value will not let the effective concurrency take higher ones. If there's a need for fixed values, *Autothrottle* and its functionality of adjusting effective parameters during crawling have to be disabled by setting ``AUTOTHROTTLE_ENABLED`` to ``False``. Under such conditions, the settings ``CONCURRENT_REQUESTS_PER_DOMAIN`` and ``DOWNLOAD_DELAY`` may be redefined with required values. But be warned, you will be doing so at your own risk -- as stated before, increasing the crawling rate results in considerably increasing the probability to being blocked by the target site. Scrapinghub advocates courtesy when crawling the sites.


DeltaFetch
==========

The purpose of this addon is to ignore requests to pages containing items seen in previous crawls of the same spider, thus producing a *delta crawl* containing only new items. For more details on the algorithm, you can check `DeltaFetch code`_.

There are two main setting for controlling this addon:

* ``DELTAFETCH_ENABLED`` - enables or disables *DeltaFetch* addon (either project-wide or per spider)
* ``DELTAFETCH_RESET`` - reset the state, forgetting visited pages in previous runs (remember to remove this setting once you complete a crawl with this setting activated)

Note that this addon depends on the `DotScrapy Persistence`_ addon, so make sure it's enabled first. Also please be informed that *DeltaFetch* does not operate on **Start URLs** (without this logic, spider may not work at all)


DotScrapy Persistence
=====================

This addon keeps the content of the ``.scrapy`` directory in a persistent store, which is loaded when the spider starts and saved when the spider finishes. It allows spiders to share data between different runs, keeping a state or any kind of data that needs to be persisted.

The ``.scrapy`` directory is well known in Scrapy and a few extensions use it to keep a state between runs. The canonical way to work with the ``.scrapy`` directory is by calling the ``scrapy.utils.project.data_path`` function, as illustrated in the following example::

    from scrapy.utils.project import data_path

    mydata_path = data_path()

    # ... use mydata_path to store or read data which will be persisted among runs ...

Supported settings:

* ``DOTSCRAPY_ENABLED`` - enables or disables *DotScrapy* addon (either project-wide or per spider)


Images
======

This addon downloads images from extracted image URLs and stores them into an Amazon S3 storage. The addon is enabled by updating the ``IMAGES_STORE`` setting and defining two item fields:

* ``image_urls`` with type *image*, which is used for annotating image URLs in the template. This will be the source field from which the addon will get URLs of the images to be downloaded.
* ``images``, where the addon will save important information about the stored image, including Amazon S3 path relative to the ``IMAGES_STORE`` setting and the original image URL. The type of this field doesn't matter, as it is defined by the pipeline, not by the *AS* extraction algorithm. That said, be sure that it is **NOT** flagged as required, otherwise the data will not be extracted, because you will not be able to annotate this field in the templates.

Those field names are the default ones, but can be overriden with the settings ``IMAGES_URLS_FIELD`` and ``IMAGES_RESULT_FIELD``. The source and target fields defined by these two settings do not need to be different -- you can make both be the same. It will ease you from defining an additional field in the item. The addon will just overwrite the data extracted by *AS* with the data it generates (which is a dict already including the origin URL).

Settings:

* ``IMAGES_STORE`` - provide a complete Amazon S3 base path (in format *s3://<bucket name>/<base path>/*) where the images should be stored
* ``IMAGES_MIN_WIDTH`` - images with a smaller width (in pixels) are ignored (default value is ``0``)
* ``IMAGES_MIN_HEIGHT`` - images with a smaller height (in pixels) are ignored (default value is ``0``)
* ``IMAGES_EXPIRES`` - when an image is already in store, update it only when its age is older than the given value in days (default value is ``90``)
* ``IMAGES_URLS_FIELD`` - specify the item field from which the addon will read the image URLs to download/store (default value is ``image_urls``)
* ``IMAGES_RESULT_FIELD`` - specify the item field where the addon will save the stored image information (default value is ``images``)

You will also need to provide the standard ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` settings so the addon will be able to upload the images in your
Amazon S3 storage.

For more details refer to `Scrapy Images Pipeline`_, since *Images* addon is based on it.


.. _querycleaner:

Query Cleaner
=============

*Query Cleaner* addon allows to clean up the request URL GET query parameters at the output of the spider in accordance with the patterns provided by the user.

In order to enable it, use at least one of the addon-specific settings: ``QUERYCLEANER_REMOVE`` or ``QUERYCLEANER_KEEP``. The first one specifies a pattern (regular expression) that a query parameter name must match in order to be removed from the URL (all the others will be accepted). And the second one specifies a pattern that a query parameter name must match in order to be kept in the URL (all the others will be removed). You can combine both if some query parameters patterns should be kept and some should not. The *remove* pattern has precedence over the *keep* one.

Note that you can specify a list of parameter names by using the ``|`` (*OR*) regex operator. For example, the pattern ``search|login|postid`` will match query parameters *search*, *login* and *postid*. This is by far the most common usage case. And by setting ``QUERYCLEANER_REMOVE`` value to ``.*`` you can completely remove all URL queries.

Supported settings:

* ``QUERYCLEANER_REMOVE``
* ``QUERYCLEANER_KEEP``

The addon is implicitly enabled when one of these settings is provided.

Let's suppose that the spider extracts URLs like::

    http://www.example.com/product.php?pid=135&cid=12&ttda=12

We want to leave only the parameter ``pid``. To achieve this objective we can use either ``QUERYCLEANER_REMOVE`` or ``QUERYCLEANER_KEEP``. In the first case, the pattern would be ``cid|ttda``. In the second case, ``pid``. The best solution depends on a particular case, that is, how the query filters will affect any other URL that the spider is expected to extract.

.. _Scrapy: https://github.com/scrapy/scrapy
.. _DeltaFetch code:  https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py
.. _`Scrapy Autothrottle`: https://scrapy.readthedocs.org/en/latest/topics/autothrottle.html
.. _`Scrapy Images Pipeline`: http://doc.scrapy.org/en/latest/topics/images.html
